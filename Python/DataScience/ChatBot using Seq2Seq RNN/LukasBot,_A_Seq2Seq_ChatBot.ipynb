{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LukasBot, A Seq2Seq ChatBot",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DbkfyCcysDx",
        "colab_type": "code",
        "outputId": "0530e46a-f645-42e7-bb51-d1e53269ecd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3GVHJ3jadQo",
        "colab_type": "code",
        "outputId": "830f6bb7-9b9b-4b2a-c48f-14162c2f6723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 33kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.18.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.24.3)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (46.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.7.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.0.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow~=2.1.0, but you'll have tensorflow 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-federated 0.12.0 has requirement tensorflow-addons~=0.7.0, but you'll have tensorflow-addons 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eao1hR2lgziI",
        "colab_type": "code",
        "outputId": "af0d6438-2f8e-4f3c-c74b-710acbda44f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from gensim.models import Word2Vec\n",
        "import random \n",
        "import re\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/ML EFFORT/Files/lukasdata.csv')\n",
        "\n",
        "questions = list()\n",
        "answers = list()\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "  questions.append(str(df.iloc[i][1]))\n",
        "  answers.append(str(df.iloc[i][2]))\n",
        "answers_with_tags = list()\n",
        "for i in range(len(answers)):\n",
        " answers_with_tags.append(answers[i])\n",
        "\n",
        "answers = list()\n",
        "for i in range(len(answers_with_tags)):\n",
        "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')\n",
        "\n",
        "print(len(questions))\n",
        "print(len(answers))\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789')\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('VOCAB SIZE : {}'.format(VOCAB_SIZE))\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append(word)\n",
        "\n",
        "\n",
        "def tokenize(sentences):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.lower()\n",
        "        sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append(tokens)\n",
        "    return tokens_list, vocabulary\n",
        "\n",
        "\n",
        "p = tokenize(questions + answers)\n",
        "model = Word2Vec(p[0], min_count=1)\n",
        "\n",
        "embedding_matrix = np.zeros((VOCAB_SIZE, 100))\n",
        "for i in range(len(tokenizer.word_index)):\n",
        "  if vocab[i] in model:\n",
        "    embedding_matrix[i] = model[vocab[i]]\n",
        "  else:\n",
        "    embedding_matrix[i] = random.randint(1, 300)\n",
        "\n",
        "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
        "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
        "padded_questions = tf.keras.preprocessing.sequence.pad_sequences(tokenized_questions, maxlen=maxlen_questions, padding='post')\n",
        "encoder_input_data = np.array(padded_questions)\n",
        "\n",
        "print(encoder_input_data.shape, maxlen_questions)\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
        "padded_answers = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "decoder_input_data = np.array(padded_answers)\n",
        "\n",
        "print(decoder_input_data.shape, maxlen_answers)\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "for i in range(len(tokenized_answers)):\n",
        "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
        "padded_answers = tf.keras.preprocessing.sequence.pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "onehot_answers = tf.keras.utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
        "decoder_output_data = np.array(onehot_answers)\n",
        "\n",
        "print(decoder_output_data.shape)\n",
        "\n",
        "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(encoder_inputs)\n",
        "enc_dropped = tf.nn.dropout(encoder_embedding, rate=0.2)\n",
        "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(enc_dropped)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, 200, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=300, epochs=350)\n",
        "#model.save_weights('my_model_weights.h5')\n",
        "model.save('lukassave_model.tf')\n",
        "# model.load_weights('bot_model.h5')\n",
        "\n",
        "\n",
        "def make_inference_models():\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(200,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(200,))\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "    return encoder_model, decoder_model\n",
        "\n",
        "\n",
        "def str_to_tokens(sentence: str):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "      if current_word not in tokenizer.word_index:\n",
        "        continue\n",
        "      tokens_list.append(tokenizer.word_index[current_word])\n",
        "    return tf.keras.preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1138\n",
            "1138\n",
            "VOCAB SIZE : 2654\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(1138, 106) 106\n",
            "(1138, 157) 157\n",
            "(1138, 157, 2654)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 200)    530800      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/Shape (Tens [(3,)]               0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/random_unif [(None, None, 200)]  0           tf_op_layer_dropout/Shape[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/random_unif [(None, None, 200)]  0           tf_op_layer_dropout/random_unifor\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/random_unif [(None, None, 200)]  0           tf_op_layer_dropout/random_unifor\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/GreaterEqua [(None, None, 200)]  0           tf_op_layer_dropout/random_unifor\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/mul (Tensor [(None, None, 200)]  0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/Cast (Tenso [(None, None, 200)]  0           tf_op_layer_dropout/GreaterEqual[\n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_dropout/mul_1 (Tens [(None, None, 200)]  0           tf_op_layer_dropout/mul[0][0]    \n",
            "                                                                 tf_op_layer_dropout/Cast[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    530800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      tf_op_layer_dropout/mul_1[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 2654)   533454      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,236,654\n",
            "Trainable params: 2,236,654\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 1138 samples\n",
            "Epoch 1/350\n",
            "1138/1138 [==============================] - 31s 27ms/sample - loss: 0.5319\n",
            "Epoch 2/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.4260\n",
            "Epoch 3/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3995\n",
            "Epoch 4/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3956\n",
            "Epoch 5/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3937\n",
            "Epoch 6/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3919\n",
            "Epoch 7/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3907\n",
            "Epoch 8/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3888\n",
            "Epoch 9/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3868\n",
            "Epoch 10/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3849\n",
            "Epoch 11/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3821\n",
            "Epoch 12/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3801\n",
            "Epoch 13/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3767\n",
            "Epoch 14/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3744\n",
            "Epoch 15/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3714\n",
            "Epoch 16/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3692\n",
            "Epoch 17/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3670\n",
            "Epoch 18/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3652\n",
            "Epoch 19/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3631\n",
            "Epoch 20/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3611\n",
            "Epoch 21/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3597\n",
            "Epoch 22/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3579\n",
            "Epoch 23/350\n",
            "1138/1138 [==============================] - 24s 22ms/sample - loss: 0.3563\n",
            "Epoch 24/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3545\n",
            "Epoch 25/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3531\n",
            "Epoch 26/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3516\n",
            "Epoch 27/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3500\n",
            "Epoch 28/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3492\n",
            "Epoch 29/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3474\n",
            "Epoch 30/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3460\n",
            "Epoch 31/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3449\n",
            "Epoch 32/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3435\n",
            "Epoch 33/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3422\n",
            "Epoch 34/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3410\n",
            "Epoch 35/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3397\n",
            "Epoch 36/350\n",
            "1138/1138 [==============================] - 24s 22ms/sample - loss: 0.3390\n",
            "Epoch 37/350\n",
            "1138/1138 [==============================] - 24s 22ms/sample - loss: 0.3377\n",
            "Epoch 38/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3364\n",
            "Epoch 39/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3437\n",
            "Epoch 40/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3343\n",
            "Epoch 41/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3333\n",
            "Epoch 42/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3325\n",
            "Epoch 43/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3313\n",
            "Epoch 44/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3302\n",
            "Epoch 45/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.3293\n",
            "Epoch 46/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3282\n",
            "Epoch 47/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3274\n",
            "Epoch 48/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3263\n",
            "Epoch 49/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.3251\n",
            "Epoch 50/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3244\n",
            "Epoch 51/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3232\n",
            "Epoch 52/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3223\n",
            "Epoch 53/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3214\n",
            "Epoch 54/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3201\n",
            "Epoch 55/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3193\n",
            "Epoch 56/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3182\n",
            "Epoch 57/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3172\n",
            "Epoch 58/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3163\n",
            "Epoch 59/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3152\n",
            "Epoch 60/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3142\n",
            "Epoch 61/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3133\n",
            "Epoch 62/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3122\n",
            "Epoch 63/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3112\n",
            "Epoch 64/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3100\n",
            "Epoch 65/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3091\n",
            "Epoch 66/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3082\n",
            "Epoch 67/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3071\n",
            "Epoch 68/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3060\n",
            "Epoch 69/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3049\n",
            "Epoch 70/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.3037\n",
            "Epoch 71/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3029\n",
            "Epoch 72/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3019\n",
            "Epoch 73/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.3009\n",
            "Epoch 74/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.3001\n",
            "Epoch 75/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2988\n",
            "Epoch 76/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2977\n",
            "Epoch 77/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2969\n",
            "Epoch 78/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2955\n",
            "Epoch 79/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2947\n",
            "Epoch 80/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2935\n",
            "Epoch 81/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2923\n",
            "Epoch 82/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2913\n",
            "Epoch 83/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2904\n",
            "Epoch 84/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2894\n",
            "Epoch 85/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2881\n",
            "Epoch 86/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2872\n",
            "Epoch 87/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2864\n",
            "Epoch 88/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2848\n",
            "Epoch 89/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2839\n",
            "Epoch 90/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2828\n",
            "Epoch 91/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2821\n",
            "Epoch 92/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2808\n",
            "Epoch 93/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2796\n",
            "Epoch 94/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.2785\n",
            "Epoch 95/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2773\n",
            "Epoch 96/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2765\n",
            "Epoch 97/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2753\n",
            "Epoch 98/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2741\n",
            "Epoch 99/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2731\n",
            "Epoch 100/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2724\n",
            "Epoch 101/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2709\n",
            "Epoch 102/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2699\n",
            "Epoch 103/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2686\n",
            "Epoch 104/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2676\n",
            "Epoch 105/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2666\n",
            "Epoch 106/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2656\n",
            "Epoch 107/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2644\n",
            "Epoch 108/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.2632\n",
            "Epoch 109/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2623\n",
            "Epoch 110/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2613\n",
            "Epoch 111/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2598\n",
            "Epoch 112/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2588\n",
            "Epoch 113/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2577\n",
            "Epoch 114/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2570\n",
            "Epoch 115/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2557\n",
            "Epoch 116/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2549\n",
            "Epoch 117/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2534\n",
            "Epoch 118/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2524\n",
            "Epoch 119/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2515\n",
            "Epoch 120/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2502\n",
            "Epoch 121/350\n",
            "1138/1138 [==============================] - 25s 22ms/sample - loss: 0.2492\n",
            "Epoch 122/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2484\n",
            "Epoch 123/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2472\n",
            "Epoch 124/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2457\n",
            "Epoch 125/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2446\n",
            "Epoch 126/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2441\n",
            "Epoch 127/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2426\n",
            "Epoch 128/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2419\n",
            "Epoch 129/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2409\n",
            "Epoch 130/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2393\n",
            "Epoch 131/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2384\n",
            "Epoch 132/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2374\n",
            "Epoch 133/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2364\n",
            "Epoch 134/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2354\n",
            "Epoch 135/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2344\n",
            "Epoch 136/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2331\n",
            "Epoch 137/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2321\n",
            "Epoch 138/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2311\n",
            "Epoch 139/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2299\n",
            "Epoch 140/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2290\n",
            "Epoch 141/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2283\n",
            "Epoch 142/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2269\n",
            "Epoch 143/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2258\n",
            "Epoch 144/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2249\n",
            "Epoch 145/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2235\n",
            "Epoch 146/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.2225\n",
            "Epoch 147/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2219\n",
            "Epoch 148/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2208\n",
            "Epoch 149/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2197\n",
            "Epoch 150/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2187\n",
            "Epoch 151/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2176\n",
            "Epoch 152/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2169\n",
            "Epoch 153/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2154\n",
            "Epoch 154/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2147\n",
            "Epoch 155/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2133\n",
            "Epoch 156/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2123\n",
            "Epoch 157/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2125\n",
            "Epoch 158/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2108\n",
            "Epoch 159/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2096\n",
            "Epoch 160/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2087\n",
            "Epoch 161/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2073\n",
            "Epoch 162/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2063\n",
            "Epoch 163/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2061\n",
            "Epoch 164/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.2046\n",
            "Epoch 165/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2035\n",
            "Epoch 166/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2025\n",
            "Epoch 167/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2018\n",
            "Epoch 168/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2007\n",
            "Epoch 169/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1998\n",
            "Epoch 170/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1983\n",
            "Epoch 171/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1976\n",
            "Epoch 172/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1967\n",
            "Epoch 173/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1957\n",
            "Epoch 174/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1950\n",
            "Epoch 175/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1938\n",
            "Epoch 176/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1932\n",
            "Epoch 177/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2192\n",
            "Epoch 178/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1917\n",
            "Epoch 179/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.2133\n",
            "Epoch 180/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1944\n",
            "Epoch 181/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1895\n",
            "Epoch 182/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1882\n",
            "Epoch 183/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1874\n",
            "Epoch 184/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1867\n",
            "Epoch 185/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1861\n",
            "Epoch 186/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1848\n",
            "Epoch 187/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1836\n",
            "Epoch 188/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1825\n",
            "Epoch 189/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1820\n",
            "Epoch 190/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1812\n",
            "Epoch 191/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1800\n",
            "Epoch 192/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1789\n",
            "Epoch 193/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1786\n",
            "Epoch 194/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1774\n",
            "Epoch 195/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1761\n",
            "Epoch 196/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1755\n",
            "Epoch 197/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1751\n",
            "Epoch 198/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1739\n",
            "Epoch 199/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1726\n",
            "Epoch 200/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1724\n",
            "Epoch 201/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1715\n",
            "Epoch 202/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1703\n",
            "Epoch 203/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1694\n",
            "Epoch 204/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1686\n",
            "Epoch 205/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1677\n",
            "Epoch 206/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1671\n",
            "Epoch 207/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1660\n",
            "Epoch 208/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1649\n",
            "Epoch 209/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1651\n",
            "Epoch 210/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1636\n",
            "Epoch 211/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1625\n",
            "Epoch 212/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1618\n",
            "Epoch 213/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1613\n",
            "Epoch 214/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1605\n",
            "Epoch 215/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1598\n",
            "Epoch 216/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1589\n",
            "Epoch 217/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1577\n",
            "Epoch 218/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1570\n",
            "Epoch 219/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1588\n",
            "Epoch 220/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1560\n",
            "Epoch 221/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1545\n",
            "Epoch 222/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1539\n",
            "Epoch 223/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1529\n",
            "Epoch 224/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1523\n",
            "Epoch 225/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1522\n",
            "Epoch 226/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1509\n",
            "Epoch 227/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1501\n",
            "Epoch 228/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1496\n",
            "Epoch 229/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1484\n",
            "Epoch 230/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1481\n",
            "Epoch 231/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1470\n",
            "Epoch 232/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1463\n",
            "Epoch 233/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1458\n",
            "Epoch 234/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1446\n",
            "Epoch 235/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1442\n",
            "Epoch 236/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1433\n",
            "Epoch 237/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1436\n",
            "Epoch 238/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1417\n",
            "Epoch 239/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1408\n",
            "Epoch 240/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1407\n",
            "Epoch 241/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1402\n",
            "Epoch 242/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1389\n",
            "Epoch 243/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1386\n",
            "Epoch 244/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1374\n",
            "Epoch 245/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1368\n",
            "Epoch 246/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1362\n",
            "Epoch 247/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1355\n",
            "Epoch 248/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1355\n",
            "Epoch 249/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1341\n",
            "Epoch 250/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1334\n",
            "Epoch 251/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1325\n",
            "Epoch 252/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1322\n",
            "Epoch 253/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1316\n",
            "Epoch 254/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1305\n",
            "Epoch 255/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1301\n",
            "Epoch 256/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1303\n",
            "Epoch 257/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1283\n",
            "Epoch 258/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1281\n",
            "Epoch 259/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1277\n",
            "Epoch 260/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1269\n",
            "Epoch 261/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1256\n",
            "Epoch 262/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1254\n",
            "Epoch 263/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1250\n",
            "Epoch 264/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1248\n",
            "Epoch 265/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1230\n",
            "Epoch 266/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1230\n",
            "Epoch 267/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1223\n",
            "Epoch 268/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1218\n",
            "Epoch 269/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1214\n",
            "Epoch 270/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1200\n",
            "Epoch 271/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1199\n",
            "Epoch 272/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1188\n",
            "Epoch 273/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1187\n",
            "Epoch 274/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1178\n",
            "Epoch 275/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1171\n",
            "Epoch 276/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1169\n",
            "Epoch 277/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1163\n",
            "Epoch 278/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1156\n",
            "Epoch 279/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1147\n",
            "Epoch 280/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1140\n",
            "Epoch 281/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1137\n",
            "Epoch 282/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1130\n",
            "Epoch 283/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.1121\n",
            "Epoch 284/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1119\n",
            "Epoch 285/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1123\n",
            "Epoch 286/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1106\n",
            "Epoch 287/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1108\n",
            "Epoch 288/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1092\n",
            "Epoch 289/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1086\n",
            "Epoch 290/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1090\n",
            "Epoch 291/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1087\n",
            "Epoch 292/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1071\n",
            "Epoch 293/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1061\n",
            "Epoch 294/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1062\n",
            "Epoch 295/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1061\n",
            "Epoch 296/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.1050\n",
            "Epoch 297/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1044\n",
            "Epoch 298/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1042\n",
            "Epoch 299/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1034\n",
            "Epoch 300/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1026\n",
            "Epoch 301/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1027\n",
            "Epoch 302/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1019\n",
            "Epoch 303/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1015\n",
            "Epoch 304/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.1007\n",
            "Epoch 305/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0998\n",
            "Epoch 306/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0998\n",
            "Epoch 307/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0996\n",
            "Epoch 308/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0990\n",
            "Epoch 309/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0983\n",
            "Epoch 310/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0973\n",
            "Epoch 311/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0969\n",
            "Epoch 312/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0973\n",
            "Epoch 313/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0962\n",
            "Epoch 314/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0958\n",
            "Epoch 315/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0948\n",
            "Epoch 316/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0947\n",
            "Epoch 317/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.0951\n",
            "Epoch 318/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0931\n",
            "Epoch 319/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0927\n",
            "Epoch 320/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0930\n",
            "Epoch 321/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0923\n",
            "Epoch 322/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0926\n",
            "Epoch 323/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0909\n",
            "Epoch 324/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0902\n",
            "Epoch 325/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0909\n",
            "Epoch 326/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0905\n",
            "Epoch 327/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.0898\n",
            "Epoch 328/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0886\n",
            "Epoch 329/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0882\n",
            "Epoch 330/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0884\n",
            "Epoch 331/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0878\n",
            "Epoch 332/350\n",
            "1138/1138 [==============================] - 23s 21ms/sample - loss: 0.0869\n",
            "Epoch 333/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0874\n",
            "Epoch 334/350\n",
            "1138/1138 [==============================] - 23s 20ms/sample - loss: 0.0868\n",
            "Epoch 335/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0855\n",
            "Epoch 336/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0857\n",
            "Epoch 337/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0853\n",
            "Epoch 338/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0844\n",
            "Epoch 339/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0839\n",
            "Epoch 340/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0839\n",
            "Epoch 341/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0835\n",
            "Epoch 342/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0829\n",
            "Epoch 343/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0829\n",
            "Epoch 344/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0820\n",
            "Epoch 345/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0815\n",
            "Epoch 346/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0811\n",
            "Epoch 347/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0811\n",
            "Epoch 348/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0804\n",
            "Epoch 349/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0810\n",
            "Epoch 350/350\n",
            "1138/1138 [==============================] - 24s 21ms/sample - loss: 0.0792\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: lukassave_model.tf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGfjXuMkb5tL",
        "colab_type": "code",
        "outputId": "c3be4721-e054-460d-c79a-587d4e382ffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.keras.models.load_model('/content/drive/My Drive/ML EFFORT/Files/save_model.tf')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.saving.saved_model.load.Model at 0x7fe481dab0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exG-kqNWVz3y",
        "colab_type": "code",
        "outputId": "1c24f8f9-ec8b-4d86-bd26-7a022aab2488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "for _ in range(100):\n",
        "    states_values = enc_model.predict(str_to_tokens(input('Enter question : ')))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter question : hey\n",
            " i think we do together very luan you ctrl f isit haha nvm i dont touch first i go link the websites end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL0X-N-L5Y6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "enc_save_classifier = open(\"enc_model.pickle\",\"wb\") #binary write\n",
        "pickle.dump(enc_model, enc_save_classifier)\n",
        "enc_save_classifier.close()\n",
        "\n",
        "dec_save_classifier = open(\"dec_model.pickle\",\"wb\") #binary write\n",
        "pickle.dump(dec_model, dec_save_classifier)\n",
        "dec_save_classifier.close()\n",
        "\n",
        "# #Load the save classifier \n",
        "# enc_classifier_saved = open(\"enc_model.pickle\", \"rb\") #binary read\n",
        "# classifier_load = pickle.load(enc_classifier_saved)\n",
        "# classifier_saved.close()\n",
        "\n",
        "# dec_classifier_saved = open(\"dec_model.pickle\", \"rb\") #binary read\n",
        "# classifier_load = pickle.load(dec_classifier_saved)\n",
        "# classifier_saved.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7qf4w8AaY7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('/content/drive/My Drive/ML EFFORT/Codes/my_model_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}